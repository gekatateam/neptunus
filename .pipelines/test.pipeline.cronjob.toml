[settings]
  id = "test.pipeline.cronjob"
  lines = 5
  run = true
  buffer = 1_000
  log_level = "warn"

[[keykeepers]]
  [keykeepers.self]
    alias = "self"

[[inputs]]
  [inputs.cronjob]
    location = "UTC"
    # this job is used to create table partitions
    [[inputs.cronjob.jobs]]
      name = "partition.create"
      schedule = "@every 30s"
      force = false
    # this job generates healthcheck
    [[inputs.cronjob.jobs]]
      name = "@{self:settings.id}"
      schedule = "@every 30s"
      force = true

[[processors]]
  [processors.starlark]
    log_level = "debug"
    code = '''
load("time.star", "time")
load("date.star", "date")
load("yaml.star", "yaml")
load("fs.star",   "fs")

taskfile = {}

if True:
    taskfile = yaml.loads(fs.read_file("Taskfile.yaml"))

def process(event):
    events = []

    for i in range(6):
        t = time.now() + time.hour * 24 * i
        e = newEvent("expeditions_" + t.format("2006_01_02"))
        e.setField("taskfile", taskfile["tasks"]["all"]["cmds"])
        e.setField("weekday", date.weekday_of(t))
        e.setField("month", date.month_of(t))
        event.shareTracker(e)

        result = handle(lambda: date.parse_weekday("tuesday"))
        if type(result) == "error":
            print("parsing failed: {}".format(result))
        else:
            e.setField("expected", date.weekday_of(event.getTimestamp()) == result)

        events.append(e)

    return events
    '''

[[processors]]
  [processors.log]
    log_level = "debug"
    level = "debug"
    [processors.log.serializer]
      type = "json"
      data_only = false

[[outputs]]
  [outputs.log]
    level = "info"
    [outputs.log.serializer]
      type = "json"
      data_only = false
